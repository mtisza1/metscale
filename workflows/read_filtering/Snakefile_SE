'''
Author: Phillip Brooks, Charles Reid, Chris Grahlmann
Affiliation: UC Davis Lab for Data Intensive Biology
Objective: A Snakemake workflow to process reads to produce quality trimmed data 
Date: 2018-06-08
Documentation: docs/workflow_readfilt.md
'''

#from utils import container_image_is_external, container_image_name
from common.utils  import container_image_is_external, container_image_name
from os.path import join, isfile, dirname
import os, re, gzip, io, glob, shutil
import pdb
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider

HTTP = HTTPRemoteProvider()



###################################
# Read Filtering: default config

data_dir = config['data_dir']
sing_dir = config['sing_dir']
biocontainers = config['biocontainers']
taxclass = config['taxonomic_classification']
assembly = config['assembly']
readfilt = config['read_filtering']
post_processing = config['post_processing']
workflows = config['workflows']


###################################
# Read Filtering: build rules

# Skip to the very end of the file 
# to see the high-level build rules
# that trigger cascades of workflow
# tasks.

###################################
'''
all_sample_names = workflows['samples_input_workflow']['samples']
sample_input_files = get_sample_inputs(all_sample_names, readfilt['read_patterns']['pre_trimming_glob_pattern'], readfilt["quality_trimming"]["sample_file_ext"], data_dir)
if (sample_input_files == []):
    print("Warning: No input files found!")
'''
def get_samples_filenames():
    # Code to get samples input files based on sample name and glob file pattern
    #Using user defined params get input file list
    
    all_sample_names = workflows['samples_input_workflow']['samples']
    sample_input_files = []
    sample_finished_folders = [] # Also checking for data subfolders with '_finished' appended
    for sample in all_sample_names:
        input_file_pattern = readfilt['read_patterns']['pre_trimming_glob_pattern']
        input_file_pattern = re.sub(r"\*", sample, input_file_pattern, 1)
        input_file_pattern = join(data_dir, input_file_pattern)
        finished_folder_output_pattern = post_processing['move_samples_to_dir']['output_pattern'].replace('{sample}','')
        finished_folder_pattern = input_file_pattern.replace(readfilt["quality_trimming"]["sample_file_ext"], finished_folder_output_pattern) 
        sample_input_files.extend(glob.glob(input_file_pattern))
        sample_finished_folders.extend(glob.glob(finished_folder_pattern))

    #now we need to remove file extension and data_dir
    file_names = []
    for file in sample_input_files:
        file_path_no_dir = file.replace(data_dir+'/','')
        file_names.append(file_path_no_dir.replace(readfilt["quality_trimming"]["sample_file_ext"],''))
    sample_input_files = file_names
    if (sample_input_files == [] and sample_finished_folders == []):
        print("Warning: No input files found!")
    return sample_input_files, sample_finished_folders
    
sample_input_files, sample_finished_folders = get_samples_filenames()

##################################
# Pretrimming code needed for convert_external_to_internal_format and pre_trimming_quality_assessment


fq_pre_trimmed = join(data_dir, (readfilt['read_patterns']['pre_trimming_pattern_SE']+readfilt['quality_trimming']['sample_file_ext']).format(
                        sample='{sample}'))
fq_pre_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['pre_trimming_pattern_SE']+readfilt['quality_trimming']['sample_file_ext']).format(
                        sample='{sample}'))
           
pre_trimming_inputs_sing = fq_pre_trimmed_sing                        
pre_trimming_inputs = fq_pre_trimmed


###################################
# Convert external files to internal file formats

def convert_external_input_files(wildcards):
    input_file = join(data_dir, wildcards.sample + readfilt["quality_trimming"]["sample_file_ext"])
    return input_file
    
def create_output_internal_format(wildcards):
   return fq_pre_trimmed.format(**wildcards)


rule convert_external_to_internal_format:
    '''
    Convert the external file format to our internal file format
    '''
    input:
        convert_external_input_files
    output:
        pre_trimming_inputs
    params:
        dest_fwd = create_output_internal_format,
        search_pattern = readfilt["read_patterns"]["reverse_pe_pattern_search"],
        replace_pattern = readfilt["read_patterns"]["reverse_pe_pattern_replace"],
    run:
        src_fwd_str = str(input)
        shutil.copy(src_fwd_str, params.dest_fwd)


###################################
# Read Filtering: pre trimming

trimmed_threads = readfilt['read_patterns']['threads']

target_suffix = readfilt['quality_assessment']['fastqc_suffix']
target_ext = "_%s.zip"%(target_suffix)
sample_file_ext = readfilt['quality_trimming']['sample_file_ext']
pre_trimming_output_fwd = re.sub(sample_file_ext, target_ext, fq_pre_trimmed)
pre_trimming_output_fwd_sing = re.sub(sample_file_ext, target_ext, fq_pre_trimmed_sing)
pre_trimming_outputs = pre_trimming_output_fwd
pre_trimming_outputs_sing = pre_trimming_output_fwd_sing
fastqc_image = container_image_name(biocontainers, 'fastqc')

def pre_trimming_qa_inputs(wildcards):
    # input already includes data/ prefix
    pre_inputs_wc = pre_trimming_inputs_sing.format(**wildcards)
    return pre_inputs_wc

def pre_trimming_qa_outputs(wildcards):
    # output already includes data/ prefix
    pre_outputs_wc = pre_trimming_outputs_sing.format(**wildcards)
    return pre_outputs_wc


rule pre_trimming_quality_assessment:
    """
    Perform a pre-trimming quality check of the reads from the sequencer.
    """
    input:
        pre_trimming_inputs
    output: 
        pre_trimming_outputs
    message: 
        '''--- Pre-trim quality check of trimmed data with fastqc.'''
    singularity: 
        fastqc_image
    threads: 
        trimmed_threads
    params:
        pre_trimming_output_wc = pre_trimming_qa_outputs,
        pre_trimming_input_wc = pre_trimming_qa_inputs
    shell:
        'fastqc -t {threads} '
        '/{params.pre_trimming_input_wc} '
        '-o /{sing_dir} '



###################################
# Read Filtering: quality trimming
# If you only want to substitute a subset of wildcards,
# you can leave a wildcard untouched by substituting
# the string {variable} for {variable}.
# 
# We use this trick several times in these rules.
# 

fq_fwd = join(data_dir, (readfilt['read_patterns']['pre_trimming_pattern_SE']+readfilt['quality_trimming']['sample_file_ext']).format(
                    direction=readfilt['direction_labels']['forward'],
                    sample='{sample}'))                 
fq_fwd_sing = join(sing_dir, (readfilt['read_patterns']['pre_trimming_pattern_SE']+readfilt['quality_trimming']['sample_file_ext']).format(
                    direction=readfilt['direction_labels']['forward'],
                    sample='{sample}'))
            
quality_input = fq_fwd
quality_input_sing = fq_fwd_sing

fq_fwd_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['post_trimming_pattern_SE']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}',
                        qual='{qual}'))
fq_fwd_trimmed = join(data_dir, (readfilt['read_patterns']['post_trimming_pattern_SE']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}',
                        qual='{qual}'))

                      
trim_target_ext = readfilt['quality_trimming']['trim_suffix']
sample_file_ext = readfilt['quality_trimming']['sample_file_ext']
trimmmo_threads = readfilt['adapter_file']['threads']

quality_output = fq_fwd_trimmed
quality_output_sing = fq_fwd_trimmed_sing
                 
adapter_file = join(data_dir, readfilt['adapter_file']['name'])
adapter_file_sing = join(sing_dir, readfilt['adapter_file']['name'])

quality_log = join(data_dir,'{sample}_trim{qual}_trimmomatic_pe.log')
quality_log_sing = join(sing_dir,'{sample}_trim{qual}_trimmomatic_pe.log')

trimmo_image = container_image_name(biocontainers,'trimmomatic')


def quality_trimming_quality_log_sing(wildcards):
    """Get the log file for this quality trimming param set"""
    return quality_log_sing.format(**wildcards)

def quality_trimming_qual(wildcards):
    """Get quality threshold for trimming"""
    return "{qual}".format(**wildcards)

def quality_trimming_quality_input(wildcards):
    """
    Wildcard substitution to get input files for trimming. 
    Absolute path for container: data/ becomes /data/
    """
    # input already includes data/ prefix
    quality_input_wc = quality_input_sing.format(**wildcards)
    return quality_input_wc

def quality_trimming_quality_output(wildcards):
    """
    Wildcard substitution to get input files for trimming. 
    Absolute path for container: data/ becomes /data/
    """
    # input includes data/ prefix
    quality_output_wc = quality_output_sing.format(**wildcards)
    return quality_output_wc


rule quality_trimming:
    """
    Trim reads from the sequencer by dropping low-quality reads.
    """
    input:
        quality_input, adapter_file
    output:
        quality_output
    message: 
        """--- Quality trimming read data."""
    singularity: 
        trimmo_image
    threads: 
        trimmmo_threads
    params:
        qual = quality_trimming_qual,
        quality_input_wc = quality_trimming_quality_input,
        quality_output_wc = quality_trimming_quality_output,
        quality_log_wc = quality_trimming_quality_log_sing
    log: 
        quality_log
    shell:
        'trimmomatic SE '
        '{params.quality_input_wc} '
        '{params.quality_output_wc} '
        'ILLUMINACLIP:/{adapter_file_sing}:2:30:15:1:true '
        'LEADING:{params.qual} '
        'TRAILING:{params.qual} '
        'SLIDINGWINDOW:4:{params.qual} '
        'MINLEN:25 '
        '-trimlog {params.quality_log_wc} '
        '-threads {threads}'


###################################
# Read Filtering: post trimming


fq_fwd_post_trimmed = join(data_dir, (readfilt['read_patterns']['post_trimming_pattern_SE']+readfilt['quality_trimming']['sample_file_ext']).format(
                            direction=readfilt['direction_labels']['forward'],
                            sample='{sample}',
                            qual='{qual}'))       
fq_fwd_post_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['post_trimming_pattern_SE']+readfilt['quality_trimming']['sample_file_ext']).format(
                            direction=readfilt['direction_labels']['forward'],
                            sample='{sample}',
                            qual='{qual}'))

                        

post_trimming_inputs_sing = fq_fwd_post_trimmed_sing
post_trimming_inputs = fq_fwd_post_trimmed
target_suffix = readfilt['quality_assessment']['fastqc_suffix']
target_ext = "_%s.zip"%(target_suffix)
sample_file_ext = readfilt['quality_trimming']['sample_file_ext']
post_trimming_output_fwd = re.sub(sample_file_ext, target_ext, fq_fwd_post_trimmed)
post_trimming_outputs = post_trimming_output_fwd
fastqc_image = container_image_name(biocontainers, 'fastqc')


def post_trimming_qa_inputs_sing(wildcards):
    # input already includes data/ prefix
    post_inputs_wc = post_trimming_inputs_sing.format(**wildcards)
    return post_inputs_wc

def post_trimming_qa_outputs(wildcards):
    # output already includes data/ prefix
    post_outputs_wc = post_trimming_outputs.format(**wildcards)
    return post_outputs_wc


rule post_trimming_quality_assessment:
    """
    Perform a post-trimming quality check 
    of the reads from the sequencer.
    """
    input:
        post_trimming_inputs
    output:
        post_trimming_outputs
    message: 
        '''--- Post-trim quality check of trimmed data with fastqc.'''
    singularity: 
        fastqc_image
    threads: 
        trimmed_threads 
    params:
        post_trimming_outputs_wc = post_trimming_qa_outputs,
        post_trimming_inputs_wc = post_trimming_qa_inputs_sing
    shell:
        'fastqc -t {threads} '
        '{params.post_trimming_inputs_wc} '
        '-o /{sing_dir} '



###################################
# Read Filtering: count unique reads

count_unique_reads_input = join(data_dir, readfilt['count_unique_reads']['input_pattern_SE'].format(
                        sample='{sample}', qual='{qual}'))
count_unique_reads_input_sing = join(sing_dir, readfilt['count_unique_reads']['input_pattern_SE'].format(
                        sample='{sample}', qual='{qual}'))

count_unique_reads_output =  join(data_dir, readfilt['count_unique_reads']['output_pattern_SE'])
count_unique_reads_output_sing =  join(sing_dir, readfilt['count_unique_reads']['output_pattern_SE'])
khmer_image = container_image_name(biocontainers,'khmer')


rule count_unique_reads:
    input:
      count_unique_reads_input
    output:
      count_unique_reads_output
    message:
        """--- Count unique reads."""
    singularity:
        khmer_image 
    params:
        reads_input = count_unique_reads_input_sing,
        reads_output = count_unique_reads_output_sing
    shell:
        'unique-kmers.py -k {wildcards.kmers} '
        '{params.reads_input} '
        '-R {params.reads_output}'

###################################
# Read Filtering: Low complexity filtering
# same as post trimmed inputs
# only used for seqscreen, so outputs not used elsewhere, yet

#inputs
lc_filter_reads_input = join(data_dir, readfilt['low_complexity_pattern_SE']['input_pattern'])
lc_filter_reads_input_sing = join(sing_dir, readfilt['low_complexity_pattern_SE']['input_pattern'])

#outputs
lc_filter_reads_output= join(data_dir, readfilt['low_complexity_pattern_SE']['output_pattern'])
lc_filter_reads_output_sing = join(sing_dir, readfilt['low_complexity_pattern_SE']['output_pattern'])

#html and json outs
lc_filter_html_sing = join(sing_dir, readfilt['low_complexity_pattern_SE']['html'])
lc_filter_html = join(data_dir, readfilt['low_complexity_pattern_SE']['html'])

lc_filter_json_sing = join(sing_dir, readfilt['low_complexity_pattern_SE']['json'])
lc_filter_json = join(data_dir, readfilt['low_complexity_pattern_SE']['json'])



def convert_lc_filter_input_sing(wildcards):
    return lc_filter_reads_input_sing.format(**wildcards)

def convert_lc_filter_input(wildcards):
    return lc_filter_reads_input_sing.format(**wildcards)

def convert_lc_filter_output_sing(wildcards):
    return lc_filter_reads_output_sing.format(**wildcards)

def convert_lc_filter_output(wildcards):
    return lc_filter_reads_output_sing.format(**wildcards)

#html convert
def convert_lc_filter_html_sing(wildcards):
    return lc_filter_html_sing.format(**wildcards)

def convert_lc_filter_html(wildcards):
    return lc_filter_html.format(**wildcards)

#json convert
def convert_lc_filter_json_sing(wildcards):
    return lc_filter_json_sing.format(**wildcards)

def convert_lc_filter_json(wildcards):
    return lc_filter_reads_json.format(**wildcards)

fastp_image = container_image_name(biocontainers,'fastp')

rule read_filtering_low_complexity_workflow:
    input:
        expand( lc_filter_reads_output,
            sample    = sample_input_files,
            qual      = workflows['low_complexity_workflow']['qual'],
        )


rule lc_filter_reads:
    input:
        lc_filter_reads_input
    output:
        lc_filter_reads_output
    params:
        filter_input = convert_lc_filter_input_sing,
        filter_output = convert_lc_filter_output_sing,
        filter_html = convert_lc_filter_html_sing,
        filter_json = convert_lc_filter_json_sing
    singularity: 
        fastp_image
    shell:
        "fastp -i {params.filter_input} -o {params.filter_output} --low_complexity_filter --complexity_threshold 20 --disable_quality_filtering --disable_adapter_trimming --thread {threads} --html {params.filter_html} --json {params.filter_json}"

###################################
# Read Filtering: convert fastq files to fasta

files, = glob_wildcards("data/{file}.fq.gz")

fastq_to_fasta_input = join(data_dir, readfilt['convert_fastq_fasta']['input_pattern'])
fastq_to_fasta_input_sing = join(sing_dir,readfilt['convert_fastq_fasta']['input_pattern'])
fastq_to_fasta_output = join(data_dir,readfilt['convert_fastq_fasta']['output_pattern'])
fastq_to_fasta_output_sing = join(sing_dir,readfilt['convert_fastq_fasta']['output_pattern'])

def convert_fastq_to_fasta_input_sing(wildcards):
    return fastq_to_fasta_input_sing.format(**wildcards)

def convert_fast_to_fasta_output_sing(wildcards):
    return fastq_to_fasta_output_sing.format(**wildcards)


rule convert_fastq_to_fasta:
    input:  
        fastq_to_fasta_input
    output: 
        fastq_to_fasta_output
    message: 
        """--- Convert fastq to fasta."""
    singularity:
        khmer_image
    params:
        convert_input = convert_fastq_to_fasta_input_sing,
        convert_output = convert_fast_to_fasta_output_sing
    shell:  
        "fastq-to-fasta.py -o {params.convert_output} {params.convert_input}"



###################################
# Read Filtering: sub set reads

subsample_input = quality_output
subsample_input_sing = quality_output_sing
subsample_max_reads = readfilt['subsamples']['max_reads']
subsample_percent = readfilt['subsamples']['percent']

subsample_output = join(data_dir, readfilt['subsamples']['subsample_output_pattern_SE'].format(
    percent=readfilt['subsamples']['percent'],
    sample='{sample}', qual='{qual}'))
subsample_output_sing = join(sing_dir, readfilt['subsamples']['subsample_output_pattern_SE'].format(
    percent=readfilt['subsamples']['percent'],
    sample='{sample}', qual='{qual}'))
    


def subsample_reads_inputs(wildcards):
    return subsample_input.format(**wildcards)

def subsample_reads_inputs_sing(wildcards):
    return subsample_input_sing.format(**wildcards)

def subsample_reads_output_sing(wildcards):
    return subsample_output_sing.format(**wildcards)


rule subsample_reads:
    """
    Sample from full reads
    """
    input:
        subsample_input
    output:
        subsample_output
    message:
        """--- Subsample read data."""
    singularity:
        khmer_image
    params:
        subsample_input_wc =  subsample_reads_inputs_sing,
        subsample_output_wc = subsample_reads_output_sing,
    shell:
        'TOTAL_READS="$(zcat {params.subsample_input_wc} | echo $((`wc -l`/4)))" && '
        'NUM_READS=$((TOTAL_READS / {subsample_percent})) && '
        'sample-reads-randomly.py -N $NUM_READS '
        '-M {subsample_max_reads} '
        '-o {params.subsample_output_wc} '
        '--gzip {params.subsample_input_wc} '



###################################
#read filtering: multiqc statistics

read_filtering_multiqc_output_file_sing = join(sing_dir, readfilt['multiqc']['multiqc_pattern_report_file'])
read_filtering_multiqc_output_dir = join(data_dir, readfilt['multiqc']['multiqc_pattern_report'])
read_filtering_multiqc_output_dir_sing = join(sing_dir, readfilt['multiqc']['multiqc_pattern_report'])
    
multiqc_image = container_image_name(biocontainers, 'multiqc')

def read_filtering_expand_multiqc_input(wildcards):
    """
    Return the expanded list from fastqc files
    """
    multiqc_input_pattern_fwd = join(data_dir, readfilt['multiqc']['multiqc_input_pattern'].format(
                                   direction=readfilt['direction_labels']['forward'], 
                                   sample='{sample}', qual='{qual}'))
    multiqc_input_pattern_rev = join(data_dir, readfilt['multiqc']['multiqc_input_pattern'].format(
                                   direction=readfilt['direction_labels']['reverse'], 
                                   sample='{sample}', qual='{qual}'))
    wildcards.qual = workflows['read_filtering_posttrim_workflow']['qual']
    read_filtering_multiqc_input_pattern_fwd = expand(multiqc_input_pattern_fwd, sample = wildcards.sample, qual=wildcards.qual)
    read_filtering_multiqc_input_pattern_rev = expand(multiqc_input_pattern_rev, sample = wildcards.sample, qual=wildcards.qual)
    original_reads = expand(pre_trimming_outputs, sample = wildcards.sample)
    read_filtering_inputs_sing = [read_filtering_multiqc_input_pattern_fwd, read_filtering_multiqc_input_pattern_rev]  
    multiqc_inputs = read_filtering_multiqc_input_pattern_fwd + read_filtering_multiqc_input_pattern_rev + original_reads
    return multiqc_inputs


def read_filtering_expand_multiqc_input_sing(wildcards):
    """
    Return the expanded list from fastqc files inside sing image
    """
   
    multiqc_input_pattern_fwd = join(sing_dir, readfilt['multiqc']['multiqc_input_pattern'].format(
                                   direction=readfilt['direction_labels']['forward'], 
                                   sample='{sample}', qual='{qual}'))
    multiqc_input_pattern_rev = join(sing_dir, readfilt['multiqc']['multiqc_input_pattern'].format(
                                   direction=readfilt['direction_labels']['reverse'], 
                                   sample='{sample}', qual='{qual}'))
    
    wildcards.qual = workflows['read_filtering_posttrim_workflow']['qual']
    read_filtering_multiqc_input_pattern_fwd = expand(multiqc_input_pattern_fwd, sample = wildcards.sample, qual=wildcards.qual)
    read_filtering_multiqc_input_pattern_rev = expand(multiqc_input_pattern_rev, sample = wildcards.sample, qual=wildcards.qual)
    read_filtering_inputs_sing = [read_filtering_multiqc_input_pattern_fwd, read_filtering_multiqc_input_pattern_rev]
    original_reads = pre_trimming_qa_outputs(wildcards)  #calling from read_filtering pretrim    
    original_reads_list = list(original_reads.split(" "))    
    multiqc_inputs = read_filtering_multiqc_input_pattern_fwd + read_filtering_multiqc_input_pattern_rev + original_reads_list
    return multiqc_inputs
    

rule read_filtering_statistics_multiqc:
    """
    Compute read filtering statistics with multiqc
    """
    input:
        read_filtering_expand_multiqc_input
    output:
        directory(read_filtering_multiqc_output_dir)
    params:
        input_files = read_filtering_expand_multiqc_input_sing,
        output_files = read_filtering_multiqc_output_file_sing,
        output_dir = read_filtering_multiqc_output_dir_sing
    message: 
        '--- Compiling read filtering statistics with multiqc'
    singularity:
        multiqc_image
    shell:
        'multiqc {params.input_files} -n {params.output_files} -o {params.output_dir}'
        

### ############################################
### # Read Filtering: bypass trimming, download trimmed data directly
### #
### # THIS RULE SHOULD PROBABLY BE COMMENTED OUT
### #
### # Note: either quality_trimming or download_trimmed_data
### # must be enabled, but not both. Otherwise you get conflicts
### # due to two rules producing the same output file.
### # 
### # download_trimmed_data is for testing, folks will not normally
### # have already-trimmed data to download.
### 
### post_trimmed_pattern = readfilt['read_patterns']['post_trimming_pattern']
### post_trimmed_relative_path = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'])
### 
### def download_reads_trimmed_data_url(wildcards):
###     """
###     Given a set of wildcards, return the URL where the 
###     post-trimmed reads can be downloaded (if available).
###     """
###     # Get the filename only from the relative path, and do wildcard substitution
###     post_trimmed_name = post_trimmed_pattern.format(**wildcards)
### 
###     # Get the URL where this file is available
###     read_data_url = config['files'][post_trimmed_name]
### 
###     return read_data_url
### 
### def download_reads_trimmed_data_file(wildcards):
###     """
###     Return the post-trimming file that matches the given wildcards
###     """
###     # Get the relative path and do wildcard substitution
###     post_trimming_file = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'])
###     return post_trimming_file.format(**wildcards)
### 
### 
### rule download_trimmed_data:
###     """
###     Fetch user-requested files from OSF containing trimmed reads that will be
###     used in various workflows.
### 
###     Note that this defines wildcard-based download rules, rather than
###     downloading all files all at once, to keep things flexible and fast.
###     """
###     output:
###         post_trimmed_relative_path
###     message:
###         """--- Skipping read trimming step, downloading trimmed reads directly."""
###     params:
###         trimmed_data_url = download_reads_trimmed_data_url,
###         trimmed_data_file = download_reads_trimmed_data_file
###     shell:
###         'wget -O {params.read_data_file} {params.read_data_url}'



###################################
# Read Filtering: build rules



rule read_filtering_pretrim_workflow:
    """
    Build rule: trigger the read filtering workflow
    """
    input:
        expand( pre_trimming_outputs,
                sample    = sample_input_files,
        )

rule read_filtering_posttrim_workflow:
    """
    Build rule: trigger the read filtering workflow
    """
    input:
        expand( post_trimming_outputs,
                sample    = sample_input_files,
                qual      = workflows['read_filtering_posttrim_workflow']['qual'],
        )

rule read_filtering_khmer_subsample_reads_workflow:
    '''
    Build rule: take reads input and run sample-reads-randomly
    '''
    input:
        expand( subsample_output,
                sample    = sample_input_files,
                qual      = workflows['read_filtering_khmer_subsample_interleaved_reads_workflow']['qual'],
        )

rule read_filtering_khmer_count_unique_kmers_workflow:
    '''
    Build rule: run unique-kmers python script return txt file
    '''
    input:
        expand( count_unique_reads_output,
                sample    = sample_input_files,
                qual        = workflows['read_filtering_khmer_count_unique_kmers_workflow']['qual'],
                kmers       = workflows['read_filtering_khmer_count_unique_kmers_workflow']['kmers'],
        )


rule read_filtering_multiqc_workflow:
    """
    Build rule: run multiqc stats on trimmed reads
    """
    input:
        expand( read_filtering_multiqc_output_dir,
                sample    = sample_input_files,
        )

    
rule read_filtering_fastq_to_fasta_workflow:
    '''
    build rule: convert fq.gz files to .fa files
    '''
    input:  
        expand("data/{file}.fa", file=files)
